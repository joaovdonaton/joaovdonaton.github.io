<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Notes on diffusion</title>
    <script>
        MathJax = {
          tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']]
          }
        };
    </script>
    <script type="text/javascript" id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>

    <link href='https://fonts.googleapis.com/css?family=Roboto' rel='stylesheet'>

    <link rel="stylesheet" href="../css/blogitem.css">
</head>
<body>
<a href="/" class="return-button">Return to main page</a>
<h1 style="text-align: center">Creating a score-matching diffusion model from scratch</h1>

<div class="blog-content">
    <h3>Intro</h3>
    <p>
    I had heard of diffusion models numerous times in the context of generative AI for image generation, but had never really taken interest in how
    they worked. A few months ago, I was lucky enough to stumble upon Peter Holderrieth and Ezra Erives' wonderful <a href="https://www.youtube.com/@peterholderrieth/videos">lectures</a>.
    This is an incredible introductory resource on the topic, and the accompanying lecture notes and labs are extremely well put together. The first thing
        that came to mind when I started learning about diffusion was implementing one of these models from scratch in PyTorch. The lectures and other sources
        I've stumbled upon introduce some math ideas that I thought were interesting, so I thought I'd write down these notes detailing all I've learned. In particular,
        I hope to write it in a structured manner such that I can use this as a resource for future reference, and it may serve other people. It should
        also be that I am by no means an expert on Diffusion or AI in general, however I do feel like I was able to learn a lot from my experience learning about
        these models and the math. Feel free to shoot me a message with any suggestions, corrections or critiques (contact information on this webpage).
    </p>

    <p>
        When researching about generative models (for images in particular), you'll probably hear about Generative Adversarial Networks (GANs). I won't get too into it, but
        the neat part about GANs is that they consist two models: a <i>Generator</i> network, which is responsible for generating new samples, and a <i>Discriminator</i> network,
        that is trained to be able to tell if a sample is from the generator (fake) or from the actual dataset (real). This style of adversarial training has its advantages,
        interestingly, I've seen it used for neural-network based audio compression (See <a href="https://arxiv.org/pdf/2107.03312">Google's SoundStream Codec</a>)
        However, from my understanding, these have largely been replaced by Diffusion models nowadays (e.g StableDiffusion 3, Dall-E, Sora).

        [[[MORE HERE ON diffusion background]]]]
    </p>

    <p>I'll write assuming a certain level of math background, but everything will be explained as it comes up. I'll assume you know things like: vectors, matrices,
    PDFs, gaussian distributions, calculus (maybe just like the idea of differential equations should be enough)</p>

    <h3>The basics</h3>
    <p>
        Now, onto the actual model. Our goal is to create something that can generate coherent images that are similar to the ones in our training dataset. We'll
        assume there exists a probability distribution that describes our train set: $P_{data}$. We represent each of our images' pixel values as a really high dimensional
        (long) vector $x$, which we can sample from $x \sim P_{data}$. However, a central issue here is that we cannot sample from $P_{data}$, all we have are
        samples from it. The whole idea behind diffusion is, we start at noise sampled from something like $\mathcal{N}(0, 1)$ (standard gaussian) and somehow
        move towards a sample that is around the high density areas of our $P_{data}$, which should consist of coherent images. We do this by defining a forward
        noising process, that takes an actual image from our train set, and gradually noises it, until we end up with complete noise. Then, it's possible to
        derive a reverse process, that takes us from this noise, to a sample.
    </p>

    <p>
        The forward process can be expressed in terms of a Stochastic Differential Equation (SDE):
    </p>

    $$dx = f(x, t)dt + g(t)dw$$

    <p>
        Let's look at this in parts. The $dx$ term just tells us this represents a small change in $x$ (remember $x$ starts at an actual image here).
        This equation tells us how to update $dx$. $dt$ is a small change in time, this change from sample to noise and the reverse is modelled as a function
        of time, I'll use $t=0$ as the initial time of a clean sample, and $t=1$ as the final time, where we have a noised sample. What's interesting here
        is the $dw$ term, this is what makes this equation <i>Stochastic</i>. It is defined as:
    </p>

    $$dw \sim \mathcal{N}(0, dt)$$

    <p>
        This is actually called <b>Brownian Motion</b>. In short, random noise pulled from a Gaussian Distribution centered at 0, and scaled by the change in it.
        This is pretty interesting. It makes it so that this forward process trajectory that $x$ follows as we update it has some randomness to it. If we take a
    </p>
</div>

</body>
</html>