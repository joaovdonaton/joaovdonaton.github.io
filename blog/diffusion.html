<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Notes on diffusion</title>
    <script>
        MathJax = {
          tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']]
          }
        };
    </script>
    <script type="text/javascript" id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>

    <link href='https://fonts.googleapis.com/css?family=Roboto' rel='stylesheet'>

    <link rel="stylesheet" href="../css/blogitem.css">
</head>
<body>
<a href="/" class="return-button">Return to main page</a>
<h1 style="text-align: center">Creating a score-matching diffusion model from scratch</h1>

<div class="blog-content">
    <h3>Intro</h3>
    <p>
    I had heard of diffusion models numerous times in the context of generative AI for image generation, but had never really taken interest in how
    they worked. A few months ago, I was lucky enough to stumble upon Peter Holderrieth and Ezra Erives' wonderful <a href="https://www.youtube.com/@peterholderrieth/videos">lectures</a>.
    This is an incredible introductory resource on the topic, and the accompanying lecture notes and labs are extremely well put together. The first thing
        that came to mind when I started learning about diffusion was implementing one of these models from scratch in PyTorch. The lectures and other sources
        I've stumbled upon introduce some math ideas that I thought were interesting, so I thought I'd write down these notes detailing all I've learned. In particular,
        I hope to write it in a structured manner such that I can use this as a resource for future reference, and it may serve other people. Feel free to shoot me
        a message with any suggestions, corrections or critiques (contact information on this webpage).
    </p>

    <p>
        When researching about generative models (for images in particular), you'll probably hear about Generative Adversarial Networks (GANs). I won't get too into it, but
        the neat part about GANs is that they consist two models: a <i>Generator</i> network, which is responsible for generating new samples, and a <i>Discriminator</i> network,
        that is trained to be able to tell if a sample is from the generator (fake) or from the actual dataset (real). This style of adversarial training has its advantages,
        interestingly, I've seen it used for neural-network based audio compression (See <a href="#ref1">[1]</a>).
        However, from my understanding, these have largely been replaced by Diffusion models nowadays (e.g StableDiffusion 3, Dall-E, Sora).
        GANs suffer from issues like training instability (due to this dual model approach) and mode collapse (output becomes limited to a subset of the
        actual distribution). Whereas with diffusion, common training objectives are quite simple, consisting of using some kind of Neural Network (NN) to learn to predict
        the noise, or to give us a score. Diffusion also offers a very clear mathematical framework for generating/sampling from our dataset's distribution.

    </p>

    <p>I'll be assuming a certain level of math background, but the main concepts will be explained as they come up. I'll assume familiarity with: vectors, matrices,
    PDFs, gaussian distributions, calculus (maybe just like the idea of differential equations should be enough), and obviously principles of deep learning. Also,
        keep in mind there are a few different variations of diffusion denoisers (different learning objective, discrete vs continuous definitions). My implementation
        uses a continuous-time score-matching approach, and some definitions and equations from <a href="#ref2">[2]</a>. This one
        is interesting specifically because it uses the differential equations we'll see here to build a sort of framework for different types
        of diffusion.
    </p>

    <!-- THE FORWARD PROCESS -->

    <h3>The Forward Process</h3>
    <p>
        Now, onto the actual model. Our goal is to create something that can generate coherent images that are similar to the ones in our training dataset. We'll
        assume there exists a probability distribution that describes our train set: $P_{data}$. We represent each of our images' pixel values as a really high dimensional
        vector $x \in \mathbb{R}^d$, where $x_0 \sim P_{data}$. However, a central issue here is that in practice we cannot sample from $P_{data}$. All we have are a limited number of
        samples from it. The whole idea behind diffusion denoising is, we start at noise sampled from something like $\mathcal{N}(0, I_d)$ (standard gaussian) and somehow
        move towards a sample that is around the high density areas of our $P_{data}$, which should consist of coherent images. We do this by defining a forward
        noising process, that takes an actual image from our train set, and gradually noises it, until we end up with complete noise (in a distribution that
        we can get the probability density for). Then, it's possible (though not simple) to derive a reverse process, that takes us from this noise, to a sample.
    </p>

    <div class="blog-image-container">
        <img src="images/diffusion/forward.png" alt="diffusion forward process" class="blog-image">
        <p class="blog-image-desc">Forward process in 6 discrete timesteps.</p>
    </div>

    <p>
        The forward process can be expressed in terms of a Stochastic Differential Equation (SDE):
    </p>

    $$dx = f(x, t)dt + g(t)dW$$

    <p>
        Let's look at this in parts. The $dx$ term just tells us this represents a small change in $x$ (remember $x$ starts at an actual image here).
        This equation tells us how to update $dx$. $dt$ is a small change in time, this change from sample to noise and the reverse is modelled as a function
        of time, I'll use $t=0$ as the initial time of a clean sample, and $t=1$ as the final time, where we have a noised sample. What's interesting here
        is the $dW$ term, this is what makes this equation <i>Stochastic</i>. It is defined as:
    </p>

    $$dw \sim \mathcal{N}(0, I_ddt)$$

    <p>
        This is actually called <b>Brownian Motion</b>. In short, random noise pulled from a Gaussian Distribution centered at 0, and scaled by the change in time. And it
        describes how noise is added to $x$ at eat timestep. Fun fact, in physics, brownian motion describes the randomness of the movements of particles suspended under
        certain conditions. An important property that should be clear is that the changes from $t$ to $t+1$ are completely independent of whatever happened before.
    </p>

    <p>
        We also have $f(x, t)$, the <i>drift coefficient</i> and $g(t)$ the diffusion coefficient. These are strategically defined to be functions such that we end up
        at our desired distribution at the end of the forward process. There are two common variations of this, the Variance Exploding (VE) and Variance Preserving (VP) processes.
        My first attempt a implementing this was actually using VE, but I had trouble getting it to work, so I switched to VP. One thing you'll notice with the implementation
        of these equations is that it's pretty easy to run into numerical issues (zero division, explosion).
        For the purposes of this implementation, we'll focus on VP, which is set up in a way that we always end up with $x_1 \sim \mathcal{N}(0,I_d)$
        (notice the notation, $x_1$ is sample at $t=1$, i.e after forward process). Here's the VP forward:
    </p>

    $$dx_t = -\frac{1}{2}\beta (t) x_t dt + \sqrt{\beta (t)}dW$$

    <p>
        Where we pick:
    </p>

    $$\beta (t) = \beta_{min} + t(\beta_{max} - \beta_{min})$$

    <p>
        This is called a <b>Noise Schedule</b>, here $\beta(t)$ linearly moves from $\beta_{min}$ to $\beta_{max}$ (I use 0.1 and 20 respectively, same as in
        <a href="#ref2">[2]</a>). To actually implement the
        forward process (we use it during training, we'll get there in a bit), we need to discretize it. We want to be able to pick a $t \in [0,1]$,
        and get a version of the sample $x_t$ with the appropriate amount of noise for that point in the noising. We do this by using an
        $\bar{\alpha}(t) = exp(-\int_0^t{\beta(t)}dt)$, which in short tells us how much of the original image is retained at time $t$. Basically,
        for $\beta(t)$ as $t \rightarrow 1$, we'll get a larger accumulation of noise, and have a smaller amount of the actual image remaining. Skipping the
        derivation, it turns out that the forward SDE has a solution for this:
    </p>

    $$x_t = x_0\sqrt{\bar{\alpha}(t)}+ \epsilon\sqrt{1 - \bar{\alpha}(t)}, \,\,\, \epsilon \in \mathcal{N}(0, I_d)$$

    <p>
        Which we can easily implement in PyTorch to get samples $x_t$ at any $t$ stage of noising. If, like me, you were wondering why this
        preserves variance to $1$. It's pretty simple to show: our data samples are normalized using $\frac{x-mean}{std}$, which means over our
        dataset each sample has unit variance. And obviously $Var(\epsilon)=1$. So if we want the variance of $x_t$, we can use the fact that both $x_0$ and $\epsilon$ are
        independent, and use the linearity and properties of variance to get:
    </p>

    $$Var(x_t) = \bar{\alpha}(t) Var(x_0) + (1-\bar{\alpha}(t)) Var(\epsilon) = \bar{\alpha}(t) + (1-\bar{\alpha}(t))) = 1$$

    <p>
        Great! now we have a concrete way of defining how we move from a sample from our dataset, $x_0$, to a sample that consists of noise pulled
        from a standard gaussian distribution, $x_1$. Next, we'll use these to define our reverse process and score, and elaborate on how training and
        inference make use of these.
    </p>

    <!-- END THE FORWARD PROCESS-->

    <!-- The Score-->
    <h3>
        Score Matching
    </h3>

    <p>
        Before getting to the reverse process, we must understand what "score" refers to here (this is diffusion via <i>score matching</i> after all). For
        probability density function $p_t$ ($p$ is different at every $t$ because of the forward process, we have a slightly different distribution at different $t$'s),
        our <i>conditional</i> score is defined as:
    </p>

    $$\nabla_x \, log \;p_t(x|x_0) $$

    <p>
        We can read this as "gradient of the $log$ likelihood of $x$ given $x_0$". Remember that $x_0$ is from our dataset $P_{data}$, so here our score is
        called conditional because $p_t(x|x_0)$ gives us the density (i.e how concentrated) of $x$ assuming that we are starting from $x_0$. This function is
        useful because the gradient gives us the direction of greatest ascent at a point $x$, so if we take our $x+\nabla_x \, log \;p_t(x|x_0) $, we are essentially
        moving in the direction of higher density regions of our distribution. Since, due to our forward process, this higher density region
        corresponds to a slightly less noisy version of $x$ (because it's a gaussian centered at factor of $x_0$). This works similarly to gradient descent,
        in the sense that we can start at a point, and gradually move according to the gradient until we end up close to an extreme.
    </p>

    <div class="blog-image-container">
        <img src="images/diffusion/scorepath.png" alt="moving in the direction of score" class="blog-image" style="height: 38vh; width: auto">
        <p class="blog-image-desc">2D visualization of moving in the direction of the score. The blue dot would be our $x_t$. The
        orange region corresponds to a high density for a given $p_t$</p>
    </div>

    <p>
        Now, it turns out that for our conditional score, it's not hard to derive an easily computable form for it. And I think it's kind of cool, so I will
        show the derivation. Remember we are using $x_t = x_0\sqrt{\bar{\alpha}(t)}+ \epsilon\sqrt{1 - \bar{\alpha}(t)}, \,\,\, \epsilon \in \mathcal{N}(0, I_d)$
        from before as our $x_t$ in the forward, this has mean $\mu = x_0\sqrt{\bar{\alpha}(t)}$ and variance $\sigma^2 = 1-\bar{\alpha}(t)$ (note
        that variance here is different, because we are only considering a single fixed $x_0$, which will not have variance, whereas before
        we were looking at variance of $x_0$ over all samples). So, we have $p_t=\mathcal{N}(x_0\sqrt{\bar{\alpha}(t)}, (1-\bar{\alpha}(t))I_d)$ is a gaussian.
        Since this is the case, we actually have an expressin for a conditional $p_t$, using the d-dimensional isotropic gaussian equation! (see end of <a href="#ref3">[3]</a>):
    </p>

    $$p_t(x)=(2\pi \sigma^2)^{\frac{d}{2}} exp({\frac{-||x-\mu||_2^2}{2\sigma^2}})$$

    <p>
        With this, we can take the log:
    </p>

    $$log \left( (2\pi \sigma^2)^{\frac{d}{2}} exp({\frac{-||x-\mu||_2^2}{2\sigma^2}}) \right) = {\frac{-||x-\mu||_2^2}{2\sigma^2}}$$

    <p>
        Then take the gradient w.r.t $x$ and plug in our $\mu$ and $\sigma^2$:
    </p>

    $$\nabla_x \; {\frac{-||x-\mu||_2^2}{2\sigma^2}} = {-\frac{x-\mu}{\sigma^2}} = -\frac{x- x_0 \sqrt{\bar{\alpha}(t)}}{1-\bar{\alpha}(t)}$$

    <p>
        We can rewrite our $x_t = x_0\sqrt{\bar{\alpha}(t)}+ \epsilon\sqrt{1 - \bar{\alpha}(t)}$, to show that the numerator from above is actually
        ${x_t-x_0\sqrt{\bar{\alpha}(t)}}=\epsilon {\sqrt{1 - \bar{\alpha}(t)}}$, plug in and we finally get:
    </p>

    $$\nabla_x \, log \;p_t(x|x_0) = -\frac{\epsilon\sqrt{1 - \bar{\alpha}(t)}}{1-\bar{\alpha}(t)} = \frac{-\epsilon}{\sqrt{1-\bar{\alpha}(t)}}$$

    <p>
        What we have just derived is the exact expression we'll use in our loss function. We'll get to the algorithm and loss definition. But for now,
        we understand the what and the why of the score function. This expression is also interesting because it tells us that the score can be written
        in terms of the noise $\epsilon$ in this case. In fact, it's not hard to adapt this to train a <i>noise</i> predictor network (used in
        the paper that popularized diffusion models for image generation, <a href="#ref4">[4]</a>), instead of the score network. So, to recap,
        we can now compute the exact score for our model at any point $x_t$, given that we are considering the conditional (based on a single $x_0$ "direction" sample).
    </p>

    <p>
        Now, another important definition is the idea of the <i>marginal</i> score. This is actually just $\nabla_x \, log \;p_t(x)$, i.e the
        unconditional score, this would be great to have, because instead of moving towards a single sample $x_0$, it would guide us towards the
        entire region that corresponds to being around the assumed $P_{data}$. However,
    </p>



    <!-- END The Score-->

    <!-- The Reverse Process-->
    <h3>
        The Reverse Process
    </h3>
    <!-- END The Reverse Process-->

    <h2>References</h2>

    <ol>
        <li id="ref1">
            <p>
              Zeghidour, N., Luebs, A., Copet, J., Tagliasacchi, M., Grangier, D., &amp; Aharoni, A. (2021).
              <em>SoundStream: An End-to-End Neural Audio Codec</em>.
              <a href="https://arxiv.org/pdf/2107.03312" target="_blank">arXiv:2107.03312</a>.
            </p>
        </li>
        <li id="ref2">
            <p>
              Song, Y. &amp; Ermon, S. (2020).
              <em>Score-Based Generative Modeling through Stochastic Differential Equations</em>.
              <a href="https://arxiv.org/pdf/2011.13456" target="_blank">arXiv:2011.13456</a>.
            </p>
        </li>
        <li id="ref3">
            <p>
              Holderrieth, P &amp; Erives, E. (2025).
              <em>An Introduction to Flow Matching and Diffusion Models</em>.
              <a href="https://arxiv.org/pdf/2506.02070" target="_blank">arXiv:2506.02070</a>.
            </p>
        </li>
    </ol>
</div>

</body>
</html>